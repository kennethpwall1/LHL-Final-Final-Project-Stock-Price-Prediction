{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "pd.set_option('display.max_rows', None)    # None means no limit\n",
    "pd.set_option('display.max_columns', None)    # None means no limit\n",
    "from functions_variables import *\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "from xgboost import XGBClassifier\n",
    "from ydata_profiling import ProfileReport\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/preprocessed/macro_finance_data_2014_2023.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Target Variable for TSX Index if Index has Increased from the Prior Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Index Up'] = df['TSX'] > df['TSX'].shift(1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For EDA set the date as the index to leverage time series specific methods for understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year'] = df.index.year\n",
    "df['Month'] = df.index.month\n",
    "df['Day'] = df.index.day\n",
    "df['DayofWeek'] = df.index.dayofweek\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/models/baseline2.pkl', 'wb') as file:\n",
    "    pickle.dump(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe that only includes the weekdays\n",
    "# commodities and currency are traded on weekend, but the index is only traded during the week, therefore the number of False values will be present \n",
    "# if the weekends are included and distort the percentage of index up calculations\n",
    "df = df[df['DayofWeek'] < 5]\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group index up by month\n",
    "grouped = df.groupby(['Year','Month'])['Index Up'].sum()\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the number of trading days per month\n",
    "trading_days = df.groupby(['Year', 'Month']).size()\n",
    "\n",
    "# Calculate the percentage of Index Up for each month relative to the total\n",
    "percentage_index_up = (grouped / trading_days) * 100\n",
    "\n",
    "# Combine the results into a DataFrame\n",
    "result = pd.DataFrame({\n",
    "    'Index Up Sum': grouped,\n",
    "    'Total Trading Days': trading_days,\n",
    "    'Percentage of Index Up': percentage_index_up\n",
    "})\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average percentage of index up per trading day for each month for all 10 years combined\n",
    "month_grouped = result.groupby('Month')['Percentage of Index Up'].mean()\n",
    "month_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average percentage of index up per trading day for each year for all 10 years\n",
    "year_grouped = result.groupby('Year')['Percentage of Index Up'].mean()\n",
    "year_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_grouped.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To determine if there are any specifc days on which the stock index goes up\n",
    "day_grouped = df.groupby('Day')['Index Up'].sum()\n",
    "day_grouped.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date-Time Components Conclusion\n",
    "<p>The analysis didn't provide any meaningful patterns or trends involved. The index increased slightly more on a daily basis than going down at an average of 52.4% of the time. The maximum month where the index went up was 82.6% of the time in 2019 and the minimum was 2017 September at 27%. During the Covid Pandemic the market actually performed better in 2019-2021 with the index increasing greater than 55% of the time. Monthly fluctions occured, but on average each year the results are similar across all 10 years. It appears that the middle of the month has higher than normal times where the index went up, with the number of days being the highest for those days.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = df.columns[:-5]\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in column_list:\n",
    "    plot_line_chart(df.index,df[column], column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation for all features\n",
    "correlation_matrix = df.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with just the target \n",
    "correlation_matrix['TSX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify upper triangle of the correlation matrix\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.9\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "\n",
    "# Drop those features with correlation greater than 0.9\n",
    "# Although TSX has correlation greater than 0.9 it will not be dropped\n",
    "to_drop.remove('EMA')\n",
    "to_drop.remove('TSX')\n",
    "df_reduced = df.drop(columns=to_drop)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Features dropped: {to_drop}\")\n",
    "print(f\"Remaining features: {df_reduced.columns.tolist()}\")\n",
    "print(f\"Number of features remaining: {df_reduced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation for all features\n",
    "correlation_matrix = df.corr()\n",
    "correlation_matrix\n",
    "\n",
    "# Identify upper triangle of the correlation matrix\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.9\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "\n",
    "# Drop those features with correlation greater than 0.9\n",
    "# Although TSX has correlation greater than 0.9 it will not be dropped\n",
    "to_drop.remove('EMA')\n",
    "to_drop.remove('TSX')\n",
    "df_reduced = df.drop(columns=to_drop)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Features dropped: {to_drop}\")\n",
    "print(f\"Remaining features: {df_reduced.columns.tolist()}\")\n",
    "print(f\"Number of features remaining: {df_reduced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT BASELINE DATAFRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/models/baseline2.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-based Features: Exponential Moving Average\n",
    "<p>An Exponential Moving Average (EMA) is a type of moving average that places a greater weight and significance on the most recent data points. It is commonly used in time series analysis, especially in financial markets, to smooth out data and identify trends more effectively than a simple moving average (SMA).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20 # period for the EMA. Used to track the trend of a stock over approximately one month, which is inline with other economic features.\n",
    "\n",
    "df['EMA'] = df['TSX'].ewm(span=n, adjust=False).mean()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the difference between consecutive EMA values, which essentially give the slope of the EMA\n",
    "df['EMA Slope'] = df['EMA'].diff()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMA divided by the closing price of the TSX index\n",
    "df['EMA/Close'] = df['EMA'] / df['TSX']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The difference between the EMA and the TSX index. If TSX is greater than moving average value will be positive.\n",
    "df['EMA Divergence'] = df['TSX'] - df['EMA']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Indicators: Relative Strength Index (RSI)\n",
    "<p>The Relative Strength Index (RSI) is a momentum oscillator used in technical analysis that measures the speed and change of price movements. It is typically used to identify overbought or oversold conditions in a market. The RSI oscillates between 0 and 100, making it easy to interpret and apply in various trading strategies.</p>\n",
    "<p>Below 30: The asset is generally considered oversold, indicating that it may be undervalued and due for a rebound. </p>\n",
    "<p>Above 70: The asset is generally considered overbought, indicating that it may be overvalued and due for a correction or pullback.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RSI'] = calculate_rsi(df['TSX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Indicators: Moving Average Convergence Divergence (MACD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MACD line\n",
    "df['MACD'] = df['TSX'].ewm(span=12, adjust=False).mean() - df['TSX'].ewm(span=26, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Signal line (9-day EMA of MACD) \n",
    "df['Signal Line'] = df['MACD'].ewm(span=9, adjust=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the MACD Histogram \n",
    "df['MACD Histogram'] = df['MACD'] - df['Signal Line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the unnecessary MACD line and Signal line columns from features\n",
    "df = df.drop(columns=['MACD', 'Signal Line'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Transformations: Daily Returns and Volatility\n",
    "<p>Understanding the price fluctuations of a financial asset and measuring the risk associated with the asset on a daily basis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Daily Returns\n",
    "df['Daily Return'] = df['TSX'].pct_change()  \n",
    "\n",
    "# Calculate the Rolling Standard Deviation of Daily Returns\n",
    "df['Daily Volatility'] = df['Daily Return'].rolling(window=14).std()\n",
    "\n",
    "df = df.drop(columns=['Daily Return']) # Simple returns is the same as the target variable, positive return equals index up, therefore will not include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['TSX', 'Daily Volatility']].tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all NaN values with zero for EMA, RSI and Volatility\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove correlation once EDA complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation for all features\n",
    "correlation_matrix = df.corr()\n",
    "correlation_matrix\n",
    "\n",
    "# Identify upper triangle of the correlation matrix\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.9\n",
    "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
    "\n",
    "# Drop those features with correlation greater than 0.9\n",
    "# Although TSX has correlation greater than 0.9 it will not be dropped\n",
    "#to_drop.remove('EMA')\n",
    "to_drop.remove('TSX')\n",
    "df_reduced = df.drop(columns=to_drop)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Features dropped: {to_drop}\")\n",
    "print(f\"Remaining features: {df_reduced.columns.tolist()}\")\n",
    "print(f\"Number of features remaining: {df_reduced.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate EDA Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the profiling report with time-series mode enabled and sorted by variable type\n",
    "profile = ProfileReport(df, tsmode=True, title=\"Time-Series Data Profiling Report\")\n",
    "\n",
    "# Save the report to an HTML file\n",
    "profile.to_file(\"your_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series-Split Data for Time Series Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced.drop(columns=['Index Up'])\n",
    "y = df_reduced['Index Up']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(n_splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in tss.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns) # maintain column names\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv('../data/processed/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/final/X_train.csv', index=False)\n",
    "X_test.to_csv('../data/final/X_test.csv', index=False)\n",
    "\n",
    "X_train_scaled.to_csv('../data/final/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/final/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv('../data/final/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/final/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/final/scalar.pkl', 'wb') as file:\n",
    "    pickle.dump(scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination - Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Initialize RFE with Logistic Regression and the number of features to select\n",
    "rfe = RFE(estimator=log_reg, n_features_to_select=25)\n",
    "\n",
    "# Fit RFE on the scaled training data\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the mask of selected features\n",
    "selected_features = rfe.support_\n",
    "\n",
    "# Convert boolean mask to the list of selected feature names\n",
    "selected_feature_names = X_train.columns[selected_features]\n",
    "\n",
    "# Filter the training and test data with the selected features\n",
    "X_train_selected = X_train[selected_feature_names]\n",
    "X_test_selected = X_test[selected_feature_names]\n",
    "\n",
    "# Fit Logistic Regression on the selected features\n",
    "log_reg.fit(X_train_selected, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_selected)\n",
    "\n",
    "# Calculate the F1 score\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted' for multiclass\n",
    "\n",
    "# Display results\n",
    "print(\"Selected Features:\", selected_feature_names.tolist())\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of features (F1 Score):\n",
    "\n",
    "17 - 0.644\n",
    "\n",
    "20 - 0.656\n",
    "\n",
    "25 - 0.636"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive Feature Elimination - XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier(random_state=42, eval_metric='mlogloss')\n",
    "\n",
    "# Create an RFE object, specifying the model and the number of features to select\n",
    "rfe = RFE(estimator=model, n_features_to_select=20)\n",
    "\n",
    "# Fit the RFE model on the data\n",
    "rfe.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the rankings of the features\n",
    "ranking = rfe.ranking_\n",
    "selected_features = X_train_scaled.columns[rfe.support_]\n",
    "\n",
    "print(f\"Feature Rankings: {ranking}\")\n",
    "print(f\"Selected Features: {selected_features.tolist()}\")\n",
    "\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# Train a model on the selected features\n",
    "model.fit(X_train_rfe, y_train)\n",
    "y_pred = model.predict(X_test_rfe)\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "print(\"Model F1 Score with Selected Features:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XGBoost RFE technique yielded the best results so far with an F1 score of 0.718, which is better than the previous benchmark of 0.71. Using the selected features above, we will see how the hyper paramater tuning will help improve the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of features (F1 Score):\n",
    "\n",
    "17 - 0.708\n",
    "\n",
    "20 - 0.713\n",
    "\n",
    "25 - 0.684\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Using Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plotting feature importances\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance using Random Forest')\n",
    "plt.gca().invert_yaxis() # To display the most important feature at the top\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several features that seem to be unimportant with importance values below the others using the Random Forest model feature importance selection, but I decided to test eliminating features below the DayofWeek to see how the model performs. After removing the below features I ran the dataset through all the models again , but the models didn't improve above the benchmark of 0.73 from Logistic Regression, but had a F1 Score of 0.71 from the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_scaled.drop(['Month', 'GDP', 'Housing starts','Interest_Rate', 'Unemployment', 'CPI'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = X_test_scaled.drop(['Month', 'GDP', 'Housing starts','Interest_Rate', 'Unemployment', 'CPI'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv('../data/processed/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Using XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBClassifier\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# Fit the model to the training data\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get feature importances using the 'Gain' method\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_train_scaled.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Visualize feature importances using XGBoost's built-in plot_importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_importance(xgb_model, importance_type='gain')  # Choose from 'weight', 'gain', 'cover'\n",
    "plt.title('Feature Importance by Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no clear features that are unimportant with importance values much below the others using the XGBoost model feature importance selection, but I decided to test eliminating features below the TSX importance value of 1.2 to see how the model performs. After removing the below features and running the dataset through all the models againg the Logistic Regression model improved above the benchmark of 0.73 to an F1 Score of 0.74."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = X_train_scaled.drop(['DayofWeek','DAX','Lumber Price', 'GBPCAD', 'CADJPY'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = X_test_scaled.drop(['DayofWeek','DAX','Lumber Price', 'GBPCAD', 'CADJPY'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.to_csv('../data/processed/X_train_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test_scaled.csv', index=False)\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
